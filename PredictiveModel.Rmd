```{r}
set.seed(9876)
central <- read.csv("Central2024P.csv", stringsAsFactors = TRUE)
attach(central)
# Separating dataset into training and testing data
train <- sample(1:nrow(central), 2000)
test <- (-train)

###########################
# Multiple linear regression
###########################
L3 <- lm(Price~Area*Tenure+Region+Age+Purchaser, central[train,])
summary(L3)
pred3 <- predict(L3, newdata=central[test,])
mean((pred3-central[test, "Price"])^2)

########################
# Best subset selection
########################
library(leaps)
reg1 <- regsubsets(Price~., central, nvmax = 18)
reg1sum <- summary(reg1)
reg1sum
# Comparing BIC, Cp, and adjusted R-square
bic <- which.min(reg1sum$bic)
cp <- which.min(reg1sum$cp)
rsq <- which.max(reg1sum$adjr2)
min(reg1sum$cp)
min(reg1sum$bic)
max(reg1sum$adjr2)
# Model with lowest BIC
coef(reg1, bic)
# Model with lowest Cp
coef(reg1, cp)
# Model with highest R-squared.
coef(reg1, rsq)

####################################################
# 10-fold cross validation on best subset selection
####################################################
predict.regsubsets <- function(object, newdata,id){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars]%*%coefi
}
k <- 10
folds <- sample(1:k, nrow(central), replace = TRUE)
cverrors <- matrix(NA, k, 18, dimnames = list(NULL, paste(1:18)))
for (j in 1:k) {
  best.fit <- regsubsets(Price~., data = central[folds!=j,], nvmax = 18)
  for (i in 1:18) {
    pred <- predict.regsubsets(best.fit, central[folds==j,], id = i)
    cverrors[j, i] <- mean((central$Price[folds==j]-pred)^2)
  }
}
meancv <- apply(cverrors, 2, mean)
min(meancv)
# Cv error is 0.160995.
smallmeancv <- which.min(meancv)
# Model with lowest mean cv error.
coef(reg1, smallmeancv)

#######################################
# Cross validation on ridge regression
#######################################
library(glmnet)
x <- model.matrix(Price~., central)[, -1]
y <- central$Price
centraltrain <- central[train,]
centraltest <- central[test,]
trainx <- model.matrix(Price~., centraltrain)[, -1]
trainy <- centraltrain$Price
testx <- model.matrix(Price~., centraltest)[, -1]
testy <- centraltest$Price
ridgemod <- glmnet(trainx, trainy, alpha = 0)
cvout <- cv.glmnet(trainx, trainy, alpha = 0)
lambdarr <- cvout$lambda.min
lambdarr

# Calculating test error
ridgepred <- predict(ridgemod, s = lambdarr, newx = x[test,])
mean((ridgepred-testy)^2)

# Ridge regression model
outrr <- glmnet(x, y, alpha = 0)
rrmodel <- predict(outrr, type = "coefficients", s = lambdarr)[1:19,]
rrmodel[rrmodel!=0]

################################
# Cross validation on the lasso
################################
# Finding best lambda based on cross validation
lassomod <- glmnet(trainx, trainy, alpha = 1)
cvout1 <- cv.glmnet(trainx, trainy, alpha = 1)
lambdalasso <- cvout1$lambda.min
lambdalasso

# Test error
lassopred <- predict(lassomod, s = lambdalasso, newx = x[test,])
mean((lassopred-testy)^2)

# The lasso model
outlr <- glmnet(x, y, alpha = 1)
lrmodel <- predict(outlr, type = "coefficients", s = lambdalasso)[1:19,]
lrmodel[lrmodel!=0]

################
# Decision Tree
################
library(tree)
tree1 <- tree(Price~., central, subset = train)
summary(tree1)
# Plot of initial decision tree
plot(tree1)
title("Regression tree for central data")
text(tree1, pretty = 0)

# Finding minimum nodes through cross validation
cvtree <- cv.tree(tree1)
plot(cvtree$size, cvtree$dev, type = "b", main = "Cross validation: Deviance vs Size", xlab = "Number of nodes", ylab = "Deviance")
minnodes <- cvtree$size[which.min(cvtree$dev)]

# Pruning the decision tree
prunetree <- prune.tree(tree1, best = minnodes)
plot(prunetree)
title("Pruned Regression Tree for central data")
text(prunetree, pretty = 0)
```


