```{r}
set.seed(9876)
central <- read.csv("Central2024P.csv", stringsAsFactors = TRUE)
attach(central)

# Using best subset selection
library(leaps)
reg1 <- regsubsets(Price~., central, nvmax = 18)
reg1sum <- summary(reg1)
reg1sum
# Comparing BIC, Cp, and adjusted R-square
bic <- which.min(reg1sum$bic)
cp <- which.min(reg1sum$cp)
rsq <- which.max(reg1sum$adjr2)
# Model with lowest BIC
coef(reg1, bic)
# Model with lowest Cp
coef(reg1, cp)
# Model with highest R-squared.
coef(reg1, rsq)

# Using cross validation approach on best subset selection (10-fold).
predict.regsubsets <- function(object, newdata,id){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars]%*%coefi
}
k <- 10
folds <- sample(1:k, nrow(central), replace = TRUE)
cverrors <- matrix(NA, k, 18, dimnames = list(NULL, paste(1:18)))
for (j in 1:k) {
  best.fit <- regsubsets(Price~., data = central[folds!=j,], nvmax = 18)
  for (i in 1:18) {
    pred <- predict.regsubsets(best.fit, central[folds==j,], id = i)
    cverrors[j, i] <- mean((central$Price[folds==j]-pred)^2)
  }
}
meancv <- apply(cverrors, 2, mean)
smallmeancv <- which.min(meancv)
# Model with lowest mean cv error.
coef(reg1, smallmeancv)

# Using cross validation on ridge regression with 83% of the data as training set.
library(glmnet)
x <- model.matrix(Price~., central)[, -1]
y <- central$Price
train <- sample(1:nrow(central), nrow(central)/1.2)
test <- (-train)
centraltrain <- central[train,]
centraltest <- central[test,]
trainx <- model.matrix(Price~., centraltrain)[, -1]
trainy <- centraltrain$Price
testx <- model.matrix(Price~., centraltest)[, -1]
testy <- centraltest$Price
ridgemod <- glmnet(trainx, trainy, alpha = 0)
cvout <- cv.glmnet(trainx, trainy, alpha = 0)
lambdarr <- cvout$lambda.min
lambdarr
# We get 154237.4 as the smallest lambda.
ridgepred <- predict(ridgemod, s = lambdarr, newx = x[test,])
mean((ridgepred-testy)^2)
# Test error is 675038115693.
outrr <- glmnet(x, y, alpha = 0)
rrmodel <- predict(outrr, type = "coefficients", s = lambdarr)[1:19,]
# Ridge regression model
rrmodel[rrmodel!=0]

# Using cross validation on lasso.
lassomod <- glmnet(trainx, trainy, alpha = 1)
cvout1 <- cv.glmnet(trainx, trainy, alpha = 1)
lambdalasso <- cvout1$lambda.min
lambdalasso
# We get 4392.738 as the smallest lambda.

```

